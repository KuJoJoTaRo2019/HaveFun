# Project_Training_0 
**Machine Learning**

A simple example of Classification and Regression problem.
Using data on UCI. 

Everything is like what we learned these years.
But notice the usage of 'pipeline', 'GridSearchCV', 'cross_val_score', 'dataframe.hist&plot', 'matshow', 'scatter_matrix', 'boxplot'.

Here are the links:

Classification: http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data

Regression: https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data



# Project_Training_1
**Web Crawler**

In this section, we have 3 examples of web crawler.

The first one uses 'request' and 'BeautifulSoup' to get and pick the data. Searching with a tree structure is efficient.

The second one uses 'request' and 're'. But we need to think out the regular expression by looking at the Html code.
Besides, TianMao has its own way to defense. I wanted data of all pages at first but I only got one page at last.

The third one uses 'selenium' to create a 'chromedriver'. It sounds great but actually costs me a lot of time. Because Google Colaboratory can't get the correct
 permission.(you have to use absolute path) Maybe I can solve this problem one day.




# Project_Training_2
**Deep Learning**

When I first create this project file, it made me excited. But when I really dive into it, I feel a little bit awful. 

1. Code given by others can be un-executable. You shouldn't copy them all and save it and open steam. Read each line and make your point. This time I got a numeric_gradient function, and a lambda function. These two would give us the grads. But it never worked, because formal parameters have nothing to do with actual parameters!

2. When you are going to learn something new, BETTER to start with a whole example NEVER pieces of it! God knows how many time I have wasted by matching pieces. You know, tensorflow has differient versions, always be careful when you start. Print the version!

3. Most surprising thing is, you just need to feed the model the path of our data, because your memory can never afford data of tons of GB. Just flow, is OK.

4. The structure of layers are important, and the data pre-processing is even more important! Deep Learning needs beautiful data.

The best model I have trained is about 85% test accuracy. But it's a binary classification problem, I think it's not enough.


# Project_Training_3
**Social Network Analysis**
