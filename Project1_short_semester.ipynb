{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_short_semester.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bpP2fk-qjshz",
        "2BtyZuaSophi",
        "YdbFyIk4PsAK"
      ],
      "authorship_tag": "ABX9TyMK9WCauRdWmFt5AC6D9mxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuJoJoTaRo2019/HaveFun/blob/master/Project1_short_semester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhnZF4mjnsO",
        "colab_type": "text"
      },
      "source": [
        "**Project1 is a web crawler.**\n",
        "\n",
        "It contains:\n",
        "1. Chinese University Rankings\n",
        "2. Price Compare of TaoBao Commodity\n",
        "3. Further method to crawl News Web\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpP2fk-qjshz",
        "colab_type": "text"
      },
      "source": [
        "##**Chinese University Rankings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI1tFqZX9zqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrVExW6Mg6wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2020.html'\n",
        "\n",
        "# get response from the web\n",
        "def getHtmlText(url):\n",
        "  try:\n",
        "    r = requests.get(url, timeout = 30)\n",
        "    r.raise_for_status()\n",
        "    r.encoding = r.apparent_encoding # decide encoding way by the contents\n",
        "    soup = BeautifulSoup(r.text, 'html.parser') # use BeautifulSoup to create a new object\n",
        "    return soup\n",
        "  except:\n",
        "    return 'HaHaHa, an Error occurs!' # report when failing to get\n",
        "\n",
        "# get data that we want\n",
        "def pick_data(soup):\n",
        "  temp = np.array(['rank','name','score'])\n",
        "  for child in soup.tr.find_all_next('tr'): # find every child of 'tr'\n",
        "    s1 = child.contents[1].string # rank\n",
        "    s2 = child.contents[3].string # name\n",
        "    s3 = child.contents[9].string # score\n",
        "    tem = np.array([s1,s2,s3])\n",
        "    temp = np.vstack((temp,tem)) # append one to the array\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnH6tbsPRua-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main\n",
        "soup = getHtmlText(url)\n",
        "result = pick_data(soup)\n",
        "np.savetxt('Chinese_University_Rankings.csv',result,fmt='%s',delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BtyZuaSophi",
        "colab_type": "text"
      },
      "source": [
        "##**Price Compare of TaoBao Commodity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqnZqGd5YeIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhN_EdiMpYc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://list.tmall.com/search_product.htm'\n",
        "\n",
        "keyword='switch' # search 'switch' on TianMao\n",
        "def getHtmlText_re(url):\n",
        "  try:\n",
        "      kv={'q':keyword}\n",
        "      k={'User-Agent':'baiduspider'}   \n",
        "      r=requests.get(url,params=kv,headers=k)\n",
        "      # r=requests.get(url,params=kv,headers=k)\n",
        "      r.raise_for_status()\n",
        "      # r.encoding = r.apparent_encoding\n",
        "      r.encoding = 'gbk' # use apparent_encoding will lead to a print error\n",
        "      # print(r.request.headers)\n",
        "      # print(r.request.url)\n",
        "      print('The First time to request succeed.')\n",
        "  except:\n",
        "      print('HaHaHa, an Error occurs!')\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWTz8HeHsE6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To delete <...> in store name\n",
        "def rename(get_storename):\n",
        "  flag = True\n",
        "  it = 0\n",
        "  for i in get_storename:\n",
        "    ch = ''\n",
        "    if '<' in i:\n",
        "      for j in i:\n",
        "        if j == '<':\n",
        "          flag = False\n",
        "        if flag:\n",
        "          ch += j\n",
        "        if j == '>':\n",
        "          flag = True\n",
        "      get_storename[it] = ch\n",
        "    it += 1 \n",
        "  return get_storename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU1FaGYz7bh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use re to get lots of information in one page\n",
        "def get_one_page(r_new):\n",
        "  get_price = re.findall(r'<em title=\"([0-9]+?.[0-9]+?)\">',r_new.text)\n",
        "  get_itemname = re.findall(r'target=\"_blank\" title=\"(.+?)\"',r_new.text)\n",
        "  get_itemname = get_itemname[1:]\n",
        "  get_storename = re.findall(r'target=\"_blank\">\\n(.+?)\\n</a>',r_new.text)\n",
        "  get_storename = rename(get_storename)\n",
        "  get_monthdealnum = re.findall(r'>([0-9]+?笔)</em>',r_new.text)\n",
        "  get_commentnum = re.findall(r'\">([0-9]+?|[0-9]+?.[0-9]+?万)</a></span>',r_new.text)\n",
        "  tem = np.vstack((get_itemname,get_price,get_storename,get_monthdealnum,get_commentnum))\n",
        "  return tem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qyw4ngBurKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block is so messy because I wanted to get all pages at first, but failed unluckily\n",
        "# When dealing with one single page, things are quite simple, you just need to stack the array together\n",
        "def pick_data_re(r_new):\n",
        "  temp = np.array(['item_name','price','store_name','deal_number_in_last_month','comment_number'])\n",
        "  # get_pagenum = re.findall(r'<b class=\"ui-page-s-len\">[0-9]+?/([0-9]+?)</b>',r_new.text)\n",
        "  # total_pagenum = eval(get_pagenum[0])\n",
        "  # for i in range(1,total_pagenum):\n",
        "  tem = get_one_page(r_new)\n",
        "  temp = np.vstack((temp,tem.T))\n",
        "    # break\n",
        "    # if i == total_pagenum - 1: # when last page\n",
        "    #   break \n",
        "    # if len(tem) == 0:\n",
        "    #   print('get nothing!')\n",
        "    #   break\n",
        "    # url_new = re.findall(r'<a href=\"(.+?)\" class=\"ui-page-s-next\"',r_new.text)\n",
        "    # url_new = re.findall(r'<a href=\"(.+?)\" class=\"ui-page-s-next\"',r.text)\n",
        "    # r_new = requests.get(url+url_new[0].replace('amp;',''),headers={'User-Agent':'baiduspider'})\n",
        "    # try:\n",
        "    #   url_new = 'https://list.tmall.com/search_product.htm?spm=a220m.1000858.1000724.10.157a749foPy1g4&cat=50024411&s='+str(60*i)+'&q='+keyword+'&sort=s&style=g&industryCatId=50024411&type=pc#J_Filter'\n",
        "    #   r_new = requests.get(url_new,headers=headers)\n",
        "    #   r.encoding = 'gbk'\n",
        "    #   # print('The %d time to request succeed.'%i)\n",
        "    #   print(r_new.request.url)\n",
        "    # except:\n",
        "    #   print('The %d time to request failed.'%i)\n",
        "  # print(len(get_price))\n",
        "  # print(len(get_itemname))\n",
        "  # print(len(get_storename))\n",
        "  # print(len(get_monthdealnum))\n",
        "  # print(len(get_commentnum))\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz6MqtByWJKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4cfc086-5cc9-4fbd-ac15-e47e4a132aa3"
      },
      "source": [
        "# Main\n",
        "r = getHtmlText_re(url)\n",
        "result = pick_data_re(r)\n",
        "np.savetxt('Price_Compare_of_TaoBao_Commodity.csv',result,fmt='%s',delimiter=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The First time to request succeed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdbFyIk4PsAK",
        "colab_type": "text"
      },
      "source": [
        "##**Further method to crawl News Web**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msmJSrM8RN9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e456a050-b0e1-4cad-b880-8e28befbaa5f"
      },
      "source": [
        "# install pakeage selenium\n",
        "pip install selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATPQ_-ExVV3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DownLoad chromedriver.exe from web\n",
        "import os, requests\n",
        "import zipfile\n",
        "\n",
        "url = \"https://chromedriver.storage.googleapis.com/85.0.4183.87/chromedriver_win32.zip\"\n",
        "fname = \"chromedriver_win32.zip\"\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "extracting = zipfile.ZipFile('chromedriver_win32.zip')\n",
        "extracting.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr_KaeXcP9Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use selenium to crawl web\n",
        "# basic version\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# the Web we want to crawl\n",
        "mainurl = 'http://rettc.ustb.edu.cn'\n",
        "\n",
        "# Virtual browser Options setting\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "prefs = {\"profile.managed_default_content_settings.images\":2}\n",
        "chrome_options.add_experimental_option(\"prefs\",prefs)\n",
        "\n",
        "# For unknown reason, the line below can't run correctly\n",
        "# But it can work using local verison, which changes the root to 'D:/***/chromedriver.exe'\n",
        "############################################################################\n",
        "driver = webdriver.Chrome(\"./chromedriver.exe\",options = chrome_options)\n",
        "############################################################################\n",
        "\n",
        "# Data saving\n",
        "f = open('./result.csv','a')\n",
        "f.write('News_name'+','+'News_link'+'\\n')\n",
        "def save(n,h):\n",
        "  f.write(n+','+h+'\\n')\n",
        "\n",
        "# use DOM node to get web content\n",
        "def obtain_data():\n",
        "  driver.get(mainurl)\n",
        "  driver.set_page_load_timeout(10)\n",
        "  nameList = driver.find_elements_by_css_selector('.news-wrap > .news-list > li > a')\n",
        "  for i in range(0,len(nameList)):\n",
        "    name = driver.find_elements_by_css_selector('.news-wrap > .news-list > li > a')[i].text\n",
        "    href = driver.find_elements_by_css_selector('.news-wrap > .news-list > li > a')[i].get_attribute('href')\n",
        "    print(name,href)\n",
        "    save(name,href)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}